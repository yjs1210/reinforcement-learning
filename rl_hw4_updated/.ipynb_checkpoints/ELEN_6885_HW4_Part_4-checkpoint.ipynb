{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9V-Rn_atzVm8"
   },
   "source": [
    "#ELEN 6885 Reinforcement Learning Coding Assignment (Part 4)#\n",
    "There are a lot of official and unofficial tutorials about Tensorflow, and there are also many open-source projects written in Tensorflow. You can refer to those resources according to your interest. In this part of homework 4, only knowledge of Deep Reinforcement Learning and basic programming skills will be needed.\n",
    "\n",
    "Please put your code into the block marked by\\\n",
    "\\############################\\\n",
    "\\# YOUR CODE STARTS HERE\\\n",
    "\\# YOUR CODE ENDS HERE\\\n",
    "\\############################\\\n",
    "Normally you don't need to edit anything outside of the block. If you do want to edit something, please use a similar manner to mark you edits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "id": "DESXLxAP9fLI",
    "outputId": "97271fbd-b794-4734-e3b1-4f73e220a34d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# DQN\n",
    "class DQN:\n",
    "  def __init__(\n",
    "      self,\n",
    "      actions_num,\n",
    "      state_size,\n",
    "      learning_rate = 0.001,\n",
    "      gamma = 0.99,\n",
    "      epsilon_min = 0.05,\n",
    "      epsilon_start = 0.9,\n",
    "      replace_target_iter = 300,\n",
    "      memory_size = 500,\n",
    "      batch_size = 2,\n",
    "      epsilon_increment = None,\n",
    "  ):\n",
    "      self.actions_num = actions_num\n",
    "      self.state_size = state_size\n",
    "      self.lr = learning_rate\n",
    "      self.gamma = gamma\n",
    "      self.epsilon_min = epsilon_min\n",
    "      self.replace_target_iter = replace_target_iter\n",
    "      self.memory_size = memory_size\n",
    "      self.batch_size = batch_size\n",
    "      self.epsilon_increment = epsilon_increment\n",
    "      self.epsilon = epsilon_start if epsilon_increment is not None else self.epsilon_min\n",
    "      self.save_model_path = './weights/DQN_model.ckpt'\n",
    "      self.memory_counter = 0\n",
    "\n",
    "      # learned steps counter\n",
    "      self.steps_counter = 0\n",
    "\n",
    "      # initialize memory [s, a, r, s_, done]\n",
    "      self.memory = np.zeros((self.memory_size, state_size * 2 + 3))\n",
    "\n",
    "      # build target_net and q_net\n",
    "      self.build_net()\n",
    "      t_params = tf.get_collection('target_net_params')\n",
    "      q_params = tf.get_collection('q_net_params')\n",
    "      self.replace_target = [tf.assign(t, q) for t, q in zip(t_params, q_params)]\n",
    "\n",
    "      # gpu setting\n",
    "      config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
    "      config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "      self.sess = tf.Session(config=config)\n",
    "\n",
    "      self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def build_net(self):\n",
    "    # build q_net\n",
    "    self.state = tf.placeholder(tf.float32, [None, self.state_size], name='state')\n",
    "    self.q_target = tf.placeholder(tf.float32, [None, self.actions_num], name='Q_target')\n",
    "    with tf.variable_scope('q_net'):\n",
    "      # c_names(collections_names) are the collections to store variables\n",
    "      c_names, neurons_layer_1, w_initializer, b_initializer = \\\n",
    "        ['q_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 100, \\\n",
    "        tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)\n",
    "\n",
    "      # layer 1\n",
    "      with tf.variable_scope('layer_1'):\n",
    "        w_layer_1 = tf.get_variable('w_layer_1', [self.state_size, neurons_layer_1], initializer=w_initializer, collections=c_names)\n",
    "        b_layer_1 = tf.get_variable('b_layer_1', [1, neurons_layer_1], initializer=b_initializer, collections=c_names)\n",
    "        layer_1 = tf.nn.relu(tf.matmul(self.state, w_layer_1) + b_layer_1)\n",
    "\n",
    "      # layer 2\n",
    "      with tf.variable_scope('layer_2'):\n",
    "        w_layer_2 = tf.get_variable('w_layer_2', [neurons_layer_1, self.actions_num], initializer=w_initializer, collections=c_names)\n",
    "        b_layer_2 = tf.get_variable('b_layer_2', [1, self.actions_num], initializer=b_initializer, collections=c_names)\n",
    "        self.q_value = tf.matmul(layer_1, w_layer_2) + b_layer_2\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "      self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_value))\n",
    "    with tf.variable_scope('train'):\n",
    "      self._train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "    # build target_net\n",
    "    self.state_t = tf.placeholder(tf.float32, [None, self.state_size], name='state_t')    # input\n",
    "    with tf.variable_scope('target_net'):\n",
    "      # c_names(collections_names) are the collections to store variables\n",
    "      c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "      # layer 1\n",
    "      with tf.variable_scope('layer_1'):\n",
    "        w_layer_1 = tf.get_variable('w_layer_1', [self.state_size, neurons_layer_1], initializer=w_initializer, collections=c_names)\n",
    "        b_layer_1 = tf.get_variable('b_layer_1', [1, neurons_layer_1], initializer=b_initializer, collections=c_names)\n",
    "        layer_1 = tf.nn.relu(tf.matmul(self.state_t, w_layer_1) + b_layer_1)\n",
    "\n",
    "      # layer 2\n",
    "\n",
    "      ############################\n",
    "      # YOUR CODE STARTS HERE\n",
    "    \n",
    "      # YOUR CODE ENDS HERE\n",
    "      ############################\n",
    "     \n",
    "  def store_transition(self, s, a, r, s_, done):\n",
    "    s=s.reshape(-1)\n",
    "    s_=s_.reshape(-1)\n",
    "    transition = np.hstack((s, [a, r], s_, done))\n",
    "\n",
    "    # replace the old memory with new observations\n",
    "    index = self.memory_counter % self.memory_size\n",
    "    self.memory[index, :] = transition\n",
    "\n",
    "    self.memory_counter += 1\n",
    "\n",
    "  def choose_action(self, observation):\n",
    "    # to have batch dimension when fed into tf placeholder\n",
    "    observation = observation[np.newaxis, :]\n",
    "    # epsilon-greedy\n",
    "    if np.random.uniform() > self.epsilon:\n",
    "      action_values = self.sess.run(self.q_value, feed_dict={self.state: observation})\n",
    "      action = np.argmax(action_values)\n",
    "    else:\n",
    "      action = np.random.randint(0, self.actions_num)\n",
    "    return action\n",
    "\n",
    "  def learn(self):\n",
    "    # replace target parameters every once a while\n",
    "    if self.steps_counter % self.replace_target_iter == 0:\n",
    "      self.sess.run(self.replace_target)\n",
    "\n",
    "    # sample a batch from the memory\n",
    "    if self.memory_counter > self.memory_size:\n",
    "      sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "    else:\n",
    "      sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "    batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "    q_next, q_value = self.sess.run(\n",
    "      [self.q_next, self.q_value],\n",
    "      feed_dict={\n",
    "        self.state_t: batch_memory[:, -self.state_size-1:-1],  # fixed params\n",
    "        self.state: batch_memory[:, :self.state_size],  # newest params\n",
    "      })\n",
    "\n",
    "    # calculate q_target\n",
    "    q_target = q_value.copy()\n",
    "\n",
    "    # only change the action-values of this batch, because we only calculate loss on the batch observations\n",
    "    batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "    act_index = batch_memory[:, self.state_size].astype(int)\n",
    "    reward = batch_memory[:, self.state_size + 1]\n",
    "    done = batch_memory[:, -1]\n",
    "    ############################\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    ############################\n",
    "    \n",
    "    # train q_net\n",
    "    _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                  feed_dict={self.state: batch_memory[:, :self.state_size],\n",
    "                                            self.q_target: q_target})\n",
    "    # change epsilon\n",
    "    self.epsilon = self.epsilon - self.epsilon_increment if self.epsilon > self.epsilon_min else self.epsilon_min\n",
    "    self.steps_counter += 1\n",
    "\n",
    "  def store(self):\n",
    "    saver = tf.train.Saver() \n",
    "    saver.save(self.sess, self.save_model_path)\n",
    "  \n",
    "  def restore(self):\n",
    "    saver = tf.train.Saver() \n",
    "    saver.restore(self.sess, self.save_model_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "R_Pan2x1XlxB",
    "outputId": "c7d720f0-dcbc-4f1a-d5ad-b84459fda9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "(array([ 0.04418977,  0.15403196,  0.03061739, -0.27268629]), 1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# cart pole gym environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env._max_episode_steps = 500\n",
    "# state and action space\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "# observation\n",
    "env.reset()\n",
    "# state, reward, done, info\n",
    "print(env.step(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMOz0IC6cJqt"
   },
   "outputs": [],
   "source": [
    "# play the game and train the network\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "episode_length_set = []\n",
    "tf.reset_default_graph()\n",
    "total_time_steps = 10\n",
    "\n",
    "RL = DQN(actions_num = 2, gamma = 0.99,\n",
    "         state_size = 4, epsilon_start = 1,\n",
    "         learning_rate = 1e-3, epsilon_min = 0.01,\n",
    "         replace_target_iter = 100, memory_size = 5000,\n",
    "         epsilon_increment = 0.00001,)\n",
    "\n",
    "new_state = env.reset()\n",
    "done = False\n",
    "episode_length_counter = 0\n",
    "for step in range(total_time_steps):\n",
    "  ############################\n",
    "  # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "  # YOUR CODE ENDS HERE\n",
    "  ############################\n",
    "  \n",
    "  if step > 200:\n",
    "    RL.learn()\n",
    "  episode_length_counter += 1\n",
    "  if episode_length_counter == 500:\n",
    "    RL.store()\n",
    "RL.store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCxAuNQegi2Q"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(episode_length_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KF1UKCM3rp4m",
    "outputId": "94076fef-765c-4d57-fba2-eb1714d6666c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./weights/DQN_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# test our network\n",
    "tf.reset_default_graph()\n",
    "RL = DQN(actions_num = 2, gamma = 1,\n",
    "         state_size = 4, epsilon_start = 1,\n",
    "         learning_rate = 1e-3, epsilon_min = 0,\n",
    "         replace_target_iter = 100, memory_size = 5000,\n",
    "         epsilon_increment = None,)\n",
    "# load saved parameters\n",
    "RL.restore()\n",
    "# run 100 trails and print how long can the agent hold the cart pole for each trail\n",
    "for i in range(100):\n",
    "  ############################\n",
    "  # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "  # YOUR CODE ENDS HERE\n",
    "  ############################\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxhwfe303YhK"
   },
   "source": [
    "You may find that the episode length doesn't stably improve as more training time is given. You can read chapter 3.2 of this paper https://arxiv.org/pdf/1711.07478.pdf if you are interested."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ELEN 6885 HW4 Part 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
